// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public partial class Tensor
    {
        
        /// <summary>
        /// Returns a new Tensor with data as the tensor data.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// 
        /// Warning
        /// new_tensor() always copies data. If you have a Tensor
        /// data and want to avoid a copy, use torch.Tensor.requires_grad_()
        /// or torch.Tensor.detach().
        /// If you have a numpy array and want to avoid a copy, use
        /// torch.from_numpy().
        /// 
        /// Warning
        /// When data is a tensor x, new_tensor() reads out ‘the data’ from whatever it is passed,
        /// and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach()
        /// and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).
        /// The equivalents using clone() and detach() are recommended.
        /// </summary>
        /// <param name="data">
        /// The returned Tensor copies data.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_tensor(NDarray data, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                data,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_tensor", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with fill_value.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="fill_value">
        /// the number to fill the output tensor with.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_full<T>(Shape size, T fill_value, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
                fill_value,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_full", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with uninitialized data.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_empty(Shape size, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_empty", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with 1.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="size">
        /// a list, tuple, or torch.Size of integers defining the
        /// shape of the output tensor.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_ones(Shape size, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_ones", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a Tensor of size size filled with 0.
        /// By default, the returned Tensor has the same torch.dtype and
        /// torch.device as this tensor.
        /// </summary>
        /// <param name="size">
        /// a list, tuple, or torch.Size of integers defining the
        /// shape of the output tensor.
        /// </param>
        /// <param name="dtype">
        /// the desired type of returned tensor.
        /// Default: if None, same torch.dtype as this tensor.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, same torch.device as this tensor.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor new_zeros(Shape size, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("new_zeros", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Computes the gradient of current tensor w.r.t. graph leaves.
        /// 
        /// The graph is differentiated using the chain rule. If the tensor is
        /// non-scalar (i.e. its data has more than one element) and requires
        /// gradient, the function additionally requires specifying gradient.
        /// It should be a tensor of matching type and location, that contains
        /// the gradient of the differentiated function w.r.t. self.
        /// 
        /// This function accumulates gradients in the leaves - you might need to
        /// zero them before calling it.
        /// </summary>
        /// <param name="gradient">
        /// Gradient w.r.t. the
        /// tensor. If it is a tensor, it will be automatically converted
        /// to a Tensor that does not require grad unless create_graph is True.
        /// None values can be specified for scalar Tensors or ones that
        /// don’t require grad. If a None value would be acceptable then
        /// this argument is optional.
        /// </param>
        /// <param name="retain_graph">
        /// If False, the graph used to compute
        /// the grads will be freed. Note that in nearly all cases setting
        /// this option to True is not needed and often can be worked around
        /// in a much more efficient way. Defaults to the value of
        /// create_graph.
        /// </param>
        /// <param name="create_graph">
        /// If True, graph of the derivative will
        /// be constructed, allowing to compute higher order derivative
        /// products. Defaults to False.
        /// </param>
        public void backward(Tensor gradient = null, bool? retain_graph = null, bool? create_graph = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (gradient!=null) kwargs["gradient"]=ToPython(gradient);
            if (retain_graph!=null) kwargs["retain_graph"]=ToPython(retain_graph);
            if (create_graph!=null) kwargs["create_graph"]=ToPython(create_graph);
            dynamic py = __self__.InvokeMethod("backward", pyargs, kwargs);
        }
        
        /// <summary>
        /// self.byte() is equivalent to self.to(torch.uint8). See to().
        /// </summary>
        public Tensor @byte()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("byte");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills the tensor with numbers drawn from the Cauchy distribution:
        /// 
        /// \[f(x) = \dfrac{1}{\pi} \dfrac{\sigma}{(x - \text{median})^2 + \sigma^2}\]
        /// </summary>
        public Tensor cauchy_(double median = 0, double sigma = 1, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (median!=0) kwargs["median"]=ToPython(median);
            if (sigma!=1) kwargs["sigma"]=ToPython(sigma);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("cauchy_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// self.char() is equivalent to self.to(torch.int8). See to().
        /// </summary>
        public Tensor @char()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("char");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of the self tensor. The copy has the same size and data
        /// type as self.
        /// 
        /// Note
        /// Unlike copy_(), this function is recorded in the computation graph. Gradients
        /// propagating to the cloned tensor will propagate to the original tensor.
        /// </summary>
        public Tensor clone()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("clone");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a contiguous tensor containing the same data as self tensor. If
        /// self tensor is contiguous, this function returns the self
        /// tensor.
        /// </summary>
        public Tensor contiguous()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("contiguous");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Copies the elements from src into self tensor and returns
        /// self.
        /// 
        /// The src tensor must be broadcastable
        /// with the self tensor. It may be of a different data type or reside on a
        /// different device.
        /// </summary>
        /// <param name="src">
        /// the source tensor to copy from
        /// </param>
        /// <param name="non_blocking">
        /// if True and this copy is between CPU and GPU,
        /// the copy may occur asynchronously with respect to the host. For other
        /// cases, this argument has no effect.
        /// </param>
        public Tensor copy_(Tensor src, bool non_blocking = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                src,
            });
            var kwargs=new PyDict();
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            dynamic py = __self__.InvokeMethod("copy_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of this object in CPU memory.
        /// 
        /// If this object is already in CPU memory and on the correct device,
        /// then no copy is performed and the original object is returned.
        /// </summary>
        public Tensor cpu()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("cpu");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of this object in CUDA memory.
        /// 
        /// If this object is already in CUDA memory and on the correct device,
        /// then no copy is performed and the original object is returned.
        /// </summary>
        /// <param name="device">
        /// The destination GPU device.
        /// Defaults to the current CUDA device.
        /// </param>
        /// <param name="non_blocking">
        /// If True and the source is in pinned memory,
        /// the copy will be asynchronous with respect to the host.
        /// Otherwise, the argument has no effect. Default: False.
        /// </param>
        public Tensor cuda(Device device = null, bool non_blocking = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (device!=null) kwargs["device"]=ToPython(device);
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            dynamic py = __self__.InvokeMethod("cuda", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns the address of the first element of self tensor.
        /// </summary>
        public int data_ptr()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("data_ptr");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
        /// </summary>
        public Tensor dequantize()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("dequantize");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// If self is a sparse COO tensor (i.e., with torch.sparse_coo layout),
        /// this returns a the number of dense dimensions. Otherwise, this throws an
        /// error.
        /// 
        /// See also Tensor.sparse_dim().
        /// </summary>
        public int dense_dim()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("dense_dim");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Returns a new Tensor, detached from the current graph.
        /// 
        /// The result will never require gradient.
        /// 
        /// Note
        /// Returned Tensor shares the same storage with the original one.
        /// In-place modifications on either of them will be seen, and may trigger
        /// errors in correctness checks.
        /// IMPORTANT NOTE: Previously, in-place size / stride / storage changes
        /// (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor
        /// also update the original tensor. Now, these in-place changes will not update the
        /// original tensor anymore, and will instead trigger an error.
        /// For sparse tensors:
        /// In-place indices / values changes (such as zero_ / copy_ / add_) to the
        /// returned tensor will not update the original tensor anymore, and will instead
        /// trigger an error.
        /// </summary>
        public void detach()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("detach");
        }
        
        /// <summary>
        /// Detaches the Tensor from the graph that created it, making it a leaf.
        /// Views cannot be detached in-place.
        /// </summary>
        public void detach_()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("detach_");
        }
        
        /// <summary>
        /// Returns the number of dimensions of self tensor.
        /// </summary>
        public int dim()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("dim");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// self.double() is equivalent to self.to(torch.float64). See to().
        /// </summary>
        public Tensor @double()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("double");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns the size in bytes of an individual element.
        /// </summary>
        public int element_size()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("element_size");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Returns a new view of the self tensor with singleton dimensions expanded
        /// to a larger size.
        /// 
        /// Passing -1 as the size for a dimension means not changing the size of
        /// that dimension.
        /// 
        /// Tensor can be also expanded to a larger number of dimensions, and the
        /// new ones will be appended at the front. For the new dimensions, the
        /// size cannot be set to -1.
        /// 
        /// Expanding a tensor does not allocate new memory, but only creates a
        /// new view on the existing tensor where a dimension of size one is
        /// expanded to a larger size by setting the stride to 0. Any dimension
        /// of size 1 can be expanded to an arbitrary value without allocating new
        /// memory.
        /// </summary>
        public Tensor expand(params int[] sizes)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("expand", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Expand this tensor to the same size as other.
        /// self.expand_as(other) is equivalent to self.expand(other.size()).
        /// 
        /// Please see expand() for more information about expand.
        /// </summary>
        public Tensor expand_as(Tensor other)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                other,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("expand_as", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with elements drawn from the exponential distribution:
        /// 
        /// \[f(x) = \lambda e^{-\lambda x}\]
        /// </summary>
        public Tensor exponential_(double lambd = 1, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (lambd!=1) kwargs["lambd"]=ToPython(lambd);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("exponential_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with the specified value.
        /// </summary>
        public Tensor fill_<T>(T @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("fill_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// self.float() is equivalent to self.to(torch.float32). See to().
        /// </summary>
        public Tensor @float()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("float");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with elements drawn from the geometric distribution:
        /// 
        /// \[f(X=k) = (1 - p)^{k - 1} p\]
        /// </summary>
        public Tensor geometric_(double p, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                p,
            });
            var kwargs=new PyDict();
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("geometric_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
        /// For CPU tensors, an error is thrown.
        /// </summary>
        public int get_device_nr()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("get_device_nr");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// self.half() is equivalent to self.to(torch.float16). See to().
        /// </summary>
        public Tensor half()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("half");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Accumulate the elements of tensor into the self tensor by adding
        /// to the indices in the order given in index. For example, if dim == 0
        /// and index[i] == j, then the ith row of tensor is added to the
        /// jth row of self.
        /// 
        /// The dimth dimension of tensor must have the same size as the
        /// length of index (which must be a vector), and all other dimensions must
        /// match self, or an error will be raised.
        /// 
        /// Note
        /// When using the CUDA backend, this operation may induce nondeterministic
        /// behaviour that is not easily switched off.
        /// Please see the notes on Reproducibility for background.
        /// </summary>
        /// <param name="dim">
        /// dimension along which to index
        /// </param>
        /// <param name="index">
        /// indices of tensor to select from
        /// </param>
        /// <param name="tensor">
        /// the tensor containing values to add
        /// </param>
        public Tensor index_add_(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_add_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.index_add_()
        /// </summary>
        public Tensor index_add(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_add", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Copies the elements of tensor into the self tensor by selecting
        /// the indices in the order given in index. For example, if dim == 0
        /// and index[i] == j, then the ith row of tensor is copied to the
        /// jth row of self.
        /// 
        /// The dimth dimension of tensor must have the same size as the
        /// length of index (which must be a vector), and all other dimensions must
        /// match self, or an error will be raised.
        /// </summary>
        /// <param name="dim">
        /// dimension along which to index
        /// </param>
        /// <param name="index">
        /// indices of tensor to select from
        /// </param>
        /// <param name="tensor">
        /// the tensor containing values to copy
        /// </param>
        public Tensor index_copy_(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_copy_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.index_copy_()
        /// </summary>
        public Tensor index_copy(int dim, Tensor<long> index, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_copy", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills the elements of the self tensor with value val by
        /// selecting the indices in the order given in index.
        /// </summary>
        /// <param name="dim">
        /// dimension along which to index
        /// </param>
        /// <param name="index">
        /// indices of self tensor to fill in
        /// </param>
        /// <param name="val">
        /// the value to fill with
        /// </param>
        public Tensor index_fill_(int dim, Tensor<long> index, float val)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                val,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_fill_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.index_fill_()
        /// </summary>
        public Tensor index_fill(int dim, Tensor<long> index, float @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("index_fill", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Puts values from the tensor value into the tensor self using
        /// the indices specified in indices (which is a tuple of Tensors). The
        /// expression tensor.index_put_(indices, value) is equivalent to
        /// tensor[indices] = value. Returns self.
        /// 
        /// If accumulate is True, the elements in tensor are added to
        /// self. If accumulate is False, the behavior is undefined if indices
        /// contain duplicate elements.
        /// </summary>
        /// <param name="indices">
        /// tensors used to index into self.
        /// </param>
        /// <param name="@value">
        /// tensor of same dtype as self.
        /// </param>
        /// <param name="accumulate">
        /// whether to accumulate into self
        /// </param>
        public Tensor index_put_(Tensor<long>[] indices, Tensor @value, bool accumulate = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                indices,
                @value,
            });
            var kwargs=new PyDict();
            if (accumulate!=false) kwargs["accumulate"]=ToPython(accumulate);
            dynamic py = __self__.InvokeMethod("index_put_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-place version of index_put_()
        /// </summary>
        public Tensor index_put(Tensor<long>[] indices, Tensor @value, bool accumulate = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                indices,
                @value,
            });
            var kwargs=new PyDict();
            if (accumulate!=false) kwargs["accumulate"]=ToPython(accumulate);
            dynamic py = __self__.InvokeMethod("index_put", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// If self is a sparse COO tensor (i.e., with torch.sparse_coo layout),
        /// this returns a view of the contained indices tensor. Otherwise, this throws an
        /// error.
        /// 
        /// See also Tensor.values().
        /// 
        /// Note
        /// This method can only be called on a coalesced sparse tensor. See
        /// Tensor.coalesce() for details.
        /// </summary>
        public Tensor indices()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("indices");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// self.int() is equivalent to self.to(torch.int32). See to().
        /// </summary>
        public Tensor @int()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("int");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Given a quantized Tensor,
        /// self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the
        /// underlying uint8_t values of the given Tensor.
        /// </summary>
        public Tensor int_repr()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("int_repr");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns True if self tensor is contiguous in memory in C order.
        /// </summary>
        public bool is_contiguous()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_contiguous");
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Returns True if the data type of self is a floating point data type.
        /// </summary>
        public (bool, bool) is_floating_point()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_floating_point");
            var t = py as PyTuple;
            return (ToCsharp<bool>(t[0]), ToCsharp<bool>(t[1]));
        }
        
        /// <summary>
        /// All Tensors that have requires_grad which is False will be leaf Tensors by convention.
        /// 
        /// For Tensors that have requires_grad which is True, they will be leaf Tensors if they were
        /// created by the user. This means that they are not the result of an operation and so
        /// grad_fn is None.
        /// 
        /// Only leaf Tensors will have their grad populated during a call to backward().
        /// To get grad populated for non-leaf Tensors, you can use retain_grad().
        /// </summary>
        public void is_leaf()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_leaf");
        }
        
        /// <summary>
        /// Returns true if this tensor resides in pinned memory
        /// </summary>
        public void is_pinned()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_pinned");
        }
        
        /// <summary>
        /// Returns True if this object refers to the same THTensor object from the
        /// Torch C API as the given tensor.
        /// </summary>
        public bool is_set_to(Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("is_set_to", pyargs, kwargs);
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Checks if tensor is in shared memory.
        /// 
        /// This is always True for CUDA tensors.
        /// </summary>
        public void is_shared()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_shared");
        }
        
        /// <summary>
        /// Returns True if the data type of self is a signed data type.
        /// </summary>
        public bool is_signed()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_signed");
            return ToCsharp<bool>(py);
        }
        
        public void is_sparse()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("is_sparse");
        }
        
        /// <summary>
        /// Fills self tensor with numbers samples from the log-normal distribution
        /// parameterized by the given mean \(\mu\) and standard deviation
        /// \(\sigma\). Note that mean and std are the mean and
        /// standard deviation of the underlying normal distribution, and not of the
        /// returned distribution:
        /// 
        /// \[f(x) = \dfrac{1}{x \sigma \sqrt{2\pi}}\ e^{-\frac{(\ln x - \mu)^2}{2\sigma^2}}\]
        /// </summary>
        public void log_normal_(double mean = 1, double std = 2, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (mean!=1) kwargs["mean"]=ToPython(mean);
            if (std!=2) kwargs["std"]=ToPython(std);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("log_normal_", pyargs, kwargs);
        }
        
        /// <summary>
        /// self.long() is equivalent to self.to(torch.int64). See to().
        /// </summary>
        public Tensor @long()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("long");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Applies callable for each element in self tensor and the given
        /// tensor and stores the results in self tensor. self tensor and
        /// the given tensor must be broadcastable.
        /// 
        /// The callable should have the signature:
        /// 
        /// def callable(a, b) -&gt; number
        /// </summary>
        public void map_(Tensor tensor, Delegate callable)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
                callable,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("map_", pyargs, kwargs);
        }
        
        /// <summary>
        /// Copies elements from source into self tensor at positions where
        /// the mask is one.
        /// The shape of mask must be broadcastable
        /// with the shape of the underlying tensor. The source should have at least
        /// as many elements as the number of ones in mask
        /// </summary>
        /// <param name="mask">
        /// the binary mask
        /// </param>
        /// <param name="source">
        /// the tensor to copy from
        /// </param>
        public void masked_scatter_(Tensor<byte> mask, Tensor source)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                mask,
                source,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("masked_scatter_", pyargs, kwargs);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.masked_scatter_()
        /// </summary>
        public Tensor masked_scatter(Tensor<byte> mask, Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                mask,
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("masked_scatter", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills elements of self tensor with value where mask is
        /// one. The shape of mask must be
        /// broadcastable with the shape of the underlying
        /// tensor.
        /// </summary>
        /// <param name="mask">
        /// the binary mask
        /// </param>
        /// <param name="@value">
        /// the value to fill in with
        /// </param>
        public void masked_fill_(Tensor<byte> mask, double @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                mask,
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("masked_fill_", pyargs, kwargs);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.masked_fill_()
        /// </summary>
        public Tensor masked_fill(Tensor<byte> mask, double @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                mask,
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("masked_fill", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Same as Tensor.narrow() except returning a copy rather
        /// than shared storage.  This is primarily for sparse tensors, which
        /// do not have a shared-storage narrow method.  Calling `narrow_copy
        /// with `dimemsion &gt; self.sparse_dim()` will return a copy with the
        /// relevant dense dimension narrowed, and `self.shape` updated accordingly.
        /// </summary>
        public Tensor narrow_copy(int dimension, int start, int length)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dimension,
                start,
                length,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("narrow_copy", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Alias for dim()
        /// </summary>
        public int ndimension()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("ndimension");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Alias for numel()
        /// </summary>
        public int nelement()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("nelement");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Fills self tensor with elements samples from the normal distribution
        /// parameterized by mean and std.
        /// </summary>
        public Tensor normal_(double mean = 0, double std = 1, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (mean!=0) kwargs["mean"]=ToPython(mean);
            if (std!=1) kwargs["std"]=ToPython(std);
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("normal_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns self tensor as a NumPy ndarray. This tensor and the
        /// returned ndarray share the same underlying storage. Changes to
        /// self tensor will be reflected in the ndarray and vice versa.
        /// </summary>
        public NDarray numpy()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("numpy");
            return ToCsharp<NDarray>(py);
        }
        
        /// <summary>
        /// Permute the dimensions of this tensor.
        /// </summary>
        public Tensor permute(Shape dims)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dims,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("permute", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Copies the tensor to pinned memory, if it’s not already pinned.
        /// </summary>
        public Tensor pin_memory()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("pin_memory");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Copies the elements from tensor into the positions specified by
        /// indices. For the purpose of indexing, the self tensor is treated as if
        /// it were a 1-D tensor.
        /// 
        /// If accumulate is True, the elements in tensor are added to
        /// self. If accumulate is False, the behavior is undefined if indices
        /// contain duplicate elements.
        /// </summary>
        /// <param name="indices">
        /// the indices into self
        /// </param>
        /// <param name="tensor">
        /// the tensor containing values to copy from
        /// </param>
        /// <param name="accumulate">
        /// whether to accumulate into self
        /// </param>
        public Tensor put_(Tensor<long> indices, Tensor tensor, bool accumulate = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                indices,
                tensor,
            });
            var kwargs=new PyDict();
            if (accumulate!=false) kwargs["accumulate"]=ToPython(accumulate);
            dynamic py = __self__.InvokeMethod("put_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Quantize a float Tensor using affine quantization scheme with given scale and
        /// zero_point.
        /// returns the quantized Tensor.
        /// </summary>
        public Tensor quantize_linear(double scale, double zero_point)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                scale,
                zero_point,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("quantize_linear", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Given a Tensor quantized by linear(affine) quantization,
        /// returns the scale of the underlying quantizer().
        /// </summary>
        public float q_scale()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("q_scale");
            return ToCsharp<float>(py);
        }
        
        /// <summary>
        /// Given a Tensor quantized by linear(affine) quantization,
        /// returns the zero_point of the underlying quantizer().
        /// </summary>
        public int q_zero_point()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("q_zero_point");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Fills self tensor with numbers sampled from the discrete uniform
        /// distribution over [from, to - 1]. If not specified, the values are usually
        /// only bounded by self tensor’s data type. However, for floating point
        /// types, if unspecified, range will be [0, 2^mantissa] to ensure that every
        /// value is representable. For example, torch.tensor(1, dtype=torch.double).random_()
        /// will be uniform in [0, 2^53].
        /// </summary>
        public Tensor<T> random_<T>(T @from, T to, object generator = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                @from,
                to,
            });
            var kwargs=new PyDict();
            if (generator!=null) kwargs["generator"]=ToPython(generator);
            dynamic py = __self__.InvokeMethod("random_", pyargs, kwargs);
            return ToCsharp<Tensor<T>>(py);
        }
        
        /// <summary>
        /// Registers a backward hook.
        /// 
        /// The hook will be called every time a gradient with respect to the
        /// Tensor is computed. The hook should have the following signature:
        /// 
        /// hook(grad) -&gt; Tensor or None
        /// 
        /// The hook should not modify its argument, but it can optionally return
        /// a new gradient which will be used in place of grad.
        /// 
        /// This function returns a handle with a method handle.remove()
        /// that removes the hook from the module.
        /// </summary>
        public void register_hook(Func<Tensor, Tensor> hook)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                hook,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("register_hook", pyargs, kwargs);
        }
        
        /// <summary>
        /// Repeats this tensor along the specified dimensions.
        /// 
        /// Unlike expand(), this function copies the tensor’s data.
        /// 
        /// Warning
        /// torch.repeat() behaves differently from
        /// numpy.repeat,
        /// but is more similar to
        /// numpy.tile.
        /// For the operator similar to numpy.repeat, see torch.repeat_interleave().
        /// </summary>
        public Tensor repeat(Shape sizes)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("repeat", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Is True if gradients need to be computed for this Tensor, False otherwise.
        /// 
        /// Note
        /// The fact that gradients need to be computed for a Tensor do not mean that the grad
        /// attribute will be populated, see is_leaf for more details.
        /// </summary>
        public bool requires_grad()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("requires_grad");
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Change if autograd should record operations on this tensor: sets this tensor’s
        /// requires_grad attribute in-place. Returns this tensor.
        /// 
        /// require_grad_()’s main use case is to tell autograd to begin recording
        /// operations on a Tensor tensor. If tensor has requires_grad=False
        /// (because it was obtained through a DataLoader, or required preprocessing or
        /// initialization), tensor.requires_grad_() makes it so that autograd will
        /// begin to record operations on tensor.
        /// </summary>
        public Tensor requires_grad_(bool requires_grad = true)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (requires_grad!=true) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("requires_grad_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Resizes self tensor to the specified size. If the number of elements is
        /// larger than the current storage size, then the underlying storage is resized
        /// to fit the new number of elements. If the number of elements is smaller, the
        /// underlying storage is not changed. Existing elements are preserved but any new
        /// memory is uninitialized.
        /// 
        /// Warning
        /// This is a low-level method. The storage is reinterpreted as C-contiguous,
        /// ignoring the current strides (unless the target size equals the current
        /// size, in which case the tensor is left unchanged). For most purposes, you
        /// will instead want to use view(), which checks for
        /// contiguity, or reshape(), which copies data if needed. To
        /// change the size in-place with custom strides, see set_().
        /// </summary>
        public Tensor resize_(Shape sizes)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("resize_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Resizes the self tensor to be the same size as the specified
        /// tensor. This is equivalent to self.resize_(tensor.size()).
        /// </summary>
        public Tensor resize_as_(Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("resize_as_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Enables .grad attribute for non-leaf Tensors.
        /// </summary>
        public void retain_grad()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("retain_grad");
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.scatter_()
        /// </summary>
        public Tensor scatter(int dim, Tensor<long> index, Tensor source)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                source,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("scatter", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Writes all values from the tensor src into self at the indices
        /// specified in the index tensor. For each value in src, its output
        /// index is specified by its index in src for dimension != dim and by
        /// the corresponding value in index for dimension = dim.
        /// 
        /// For a 3-D tensor, self is updated as:
        /// 
        /// self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0
        /// self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1
        /// self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2
        /// 
        /// This is the reverse operation of the manner described in gather().
        /// 
        /// self, index and src (if it is a Tensor) should have same
        /// number of dimensions. It is also required that index.size(d) &lt;= src.size(d)
        /// for all dimensions d, and that index.size(d) &lt;= self.size(d) for all
        /// dimensions d != dim.
        /// 
        /// Moreover, as for gather(), the values of index must be
        /// between 0 and self.size(dim) - 1 inclusive, and all values in a row
        /// along the specified dimension dim must be unique.
        /// </summary>
        /// <param name="dim">
        /// the axis along which to index
        /// </param>
        /// <param name="index">
        /// the indices of elements to scatter,
        /// can be either empty or the same size of src.
        /// When empty, the operation returns identity
        /// </param>
        /// <param name="src">
        /// the source element(s) to scatter,
        /// incase value is not specified
        /// </param>
        /// <param name="@value">
        /// the source element(s) to scatter,
        /// incase src is not specified
        /// </param>
        public Tensor scatter_(int dim, Tensor<long> index, Tensor src, float @value)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                src,
                @value,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("scatter_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Adds all values from the tensor other into self at the indices
        /// specified in the index tensor in a similar fashion as
        /// scatter_(). For each value in other, it is added to
        /// an index in self which is specified by its index in other
        /// for dimension != dim and by the corresponding value in index for
        /// dimension = dim.
        /// 
        /// For a 3-D tensor, self is updated as:
        /// 
        /// self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0
        /// self[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1
        /// self[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2
        /// 
        /// self, index and other should have same number of
        /// dimensions. It is also required that index.size(d) &lt;= other.size(d) for all
        /// dimensions d, and that index.size(d) &lt;= self.size(d) for all dimensions
        /// d != dim.
        /// 
        /// Moreover, as for gather(), the values of index must be
        /// between 0 and self.size(dim) - 1 inclusive, and all values in a row along
        /// the specified dimension dim must be unique.
        /// 
        /// Note
        /// When using the CUDA backend, this operation may induce nondeterministic
        /// behaviour that is not easily switched off.
        /// Please see the notes on Reproducibility for background.
        /// </summary>
        /// <param name="dim">
        /// the axis along which to index
        /// </param>
        /// <param name="index">
        /// the indices of elements to scatter and add,
        /// can be either empty or the same size of src.
        /// When empty, the operation returns identity.
        /// </param>
        /// <param name="other">
        /// the source elements to scatter and add
        /// </param>
        public Tensor scatter_add_(int dim, Tensor<long> index, Tensor other)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                other,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("scatter_add_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Out-of-place version of torch.Tensor.scatter_add_()
        /// </summary>
        public Tensor scatter_add(int dim, Tensor<long> index, Tensor source)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
                source,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("scatter_add", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Slices the self tensor along the selected dimension at the given index.
        /// This function returns a tensor with the given dimension removed.
        /// </summary>
        /// <param name="dim">
        /// the dimension to slice
        /// </param>
        /// <param name="index">
        /// the index to select with
        /// </param>
        public Tensor @select(int dim, int index)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
                index,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("select", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Sets the underlying storage, size, and strides. If source is a tensor,
        /// self tensor will share the same storage and have the same size and
        /// strides as source. Changes to elements in one tensor will be reflected
        /// in the other.
        /// 
        /// If source is a Storage, the method sets the underlying
        /// storage, offset, size, and stride.
        /// </summary>
        /// <param name="source">
        /// the tensor or storage to use
        /// </param>
        /// <param name="storage_offset">
        /// the offset in the storage
        /// </param>
        /// <param name="size">
        /// the desired size. Defaults to the size of the source.
        /// </param>
        /// <param name="stride">
        /// the desired stride. Defaults to C-contiguous strides.
        /// </param>
        public Tensor set_(Tensor source = null, int? storage_offset = 0, Shape size = null, int[] stride = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (source!=null) kwargs["source"]=ToPython(source);
            if (storage_offset!=null) kwargs["storage_offset"]=ToPython(storage_offset);
            if (size!=null) kwargs["size"]=ToPython(size);
            if (stride!=null) kwargs["stride"]=ToPython(stride);
            dynamic py = __self__.InvokeMethod("set_", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Moves the underlying storage to shared memory.
        /// 
        /// This is a no-op if the underlying storage is already in shared memory
        /// and for CUDA tensors. Tensors in shared memory cannot be resized.
        /// </summary>
        public void share_memory_()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("share_memory_");
        }
        
        /// <summary>
        /// self.short() is equivalent to self.to(torch.int16). See to().
        /// </summary>
        public Tensor @short()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("short");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns the size of the self tensor. The returned value is a subclass of
        /// tuple.
        /// </summary>
        public Shape size()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("size");
            return ToCsharp<Shape>(py);
        }
        
        /// <summary>
        /// Returns a new SparseTensor with values from Tensor input filtered
        /// by indices of mask and values are ignored. input and mask
        /// must have the same shape.
        /// </summary>
        /// <param name="input">
        /// an input Tensor
        /// </param>
        /// <param name="mask">
        /// a SparseTensor which we filter input based on its indices
        /// </param>
        public Tensor sparse_mask(Tensor input, Tensor<byte> mask)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                mask,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("sparse_mask", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// If self is a sparse COO tensor (i.e., with torch.sparse_coo layout),
        /// this returns a the number of sparse dimensions. Otherwise, this throws an
        /// error.
        /// 
        /// See also Tensor.dense_dim().
        /// </summary>
        public int sparse_dim()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("sparse_dim");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Returns the underlying storage.
        /// </summary>
        public Storage storage()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("storage");
            return ToCsharp<Storage>(py);
        }
        
        /// <summary>
        /// Returns self tensor’s offset in the underlying storage in terms of
        /// number of storage elements (not bytes).
        /// </summary>
        public int storage_offset()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("storage_offset");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Returns the type of the underlying storage.
        /// </summary>
        public Dtype storage_type()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("storage_type");
            return ToCsharp<Dtype>(py);
        }
        
        /// <summary>
        /// Returns the stride of self tensor.
        /// 
        /// Stride is the jump necessary to go from one element to the next one in the
        /// specified dimension dim. A tuple of all strides is returned when no
        /// argument is passed in. Otherwise, an integer value is returned as the stride in
        /// the particular dimension dim.
        /// </summary>
        public int[] stride()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("stride");
            return ToCsharp<int[]>(py);
        }
        
        /// <summary>
        /// Returns the stride of self tensor.
        /// 
        /// Stride is the jump necessary to go from one element to the next one in the
        /// specified dimension dim. A tuple of all strides is returned when no
        /// argument is passed in. Otherwise, an integer value is returned as the stride in
        /// the particular dimension dim.
        /// </summary>
        public int stride(int dim)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dim,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("stride", pyargs, kwargs);
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Subtracts a scalar or tensor from self tensor. If both value and
        /// other are specified, each element of other is scaled by
        /// value before being used.
        /// 
        /// When other is a tensor, the shape of other must be
        /// broadcastable with the shape of the underlying
        /// tensor.
        /// </summary>
        public Tensor sub<T>(T @value, Tensor other = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                @value,
            });
            var kwargs=new PyDict();
            if (other!=null) kwargs["other"]=ToPython(other);
            dynamic py = __self__.InvokeMethod("sub", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Sum this tensor to size.
        /// size must be broadcastable to this tensor size.
        /// :param other: The result tensor has the same size
        /// 
        /// as other.
        /// </summary>
        public Tensor sum_to_size(Shape size, Tensor other = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
            });
            var kwargs=new PyDict();
            if (other!=null) kwargs["other"]=ToPython(other);
            dynamic py = __self__.InvokeMethod("sum_to_size", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are
        /// inferred from the arguments of self.to(*args, **kwargs).
        /// 
        /// Note
        /// If the self Tensor already
        /// has the correct torch.dtype and torch.device, then self is returned.
        /// Otherwise, the returned tensor is a copy of self with the desired
        /// torch.dtype and torch.device.
        /// 
        /// Here are the ways to call to:
        /// </summary>
        public Tensor to(Device device, Dtype dtype = null, bool non_blocking = false, bool copy = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                device,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            if (copy!=false) kwargs["copy"]=ToPython(copy);
            dynamic py = __self__.InvokeMethod("to", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are
        /// inferred from the arguments of self.to(*args, **kwargs).
        /// 
        /// Note
        /// If the self Tensor already
        /// has the correct torch.dtype and torch.device, then self is returned.
        /// Otherwise, the returned tensor is a copy of self with the desired
        /// torch.dtype and torch.device.
        /// 
        /// Here are the ways to call to:
        /// </summary>
        public Tensor to(Tensor other, bool non_blocking = false, bool copy = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                other,
            });
            var kwargs=new PyDict();
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            if (copy!=false) kwargs["copy"]=ToPython(copy);
            dynamic py = __self__.InvokeMethod("to", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are
        /// inferred from the arguments of self.to(*args, **kwargs).
        /// 
        /// Note
        /// If the self Tensor already
        /// has the correct torch.dtype and torch.device, then self is returned.
        /// Otherwise, the returned tensor is a copy of self with the desired
        /// torch.dtype and torch.device.
        /// 
        /// Here are the ways to call to:
        /// </summary>
        public Tensor to(Dtype dtype, bool non_blocking = false, bool copy = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dtype,
            });
            var kwargs=new PyDict();
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            if (copy!=false) kwargs["copy"]=ToPython(copy);
            dynamic py = __self__.InvokeMethod("to", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a copy of the tensor in torch.mkldnn layout.
        /// </summary>
        public Tensor to_mkldnn()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("to_mkldnn");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// ”
        /// tolist() -&gt; list or number
        /// 
        /// Returns the tensor as a (nested) list. For scalars, a standard
        /// Python number is returned, just like with item().
        /// Tensors are automatically moved to the CPU first if necessary.
        /// 
        /// This operation is not differentiable.
        /// </summary>
        public void tolist()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("tolist");
        }
        
        /// <summary>
        /// Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in
        /// coordinate format.
        /// </summary>
        public Tensor to_sparse(int sparseDims)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sparseDims,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("to_sparse", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns the type if dtype is not provided, else casts this object to
        /// the specified type.
        /// 
        /// If this is already of the correct type, no copy is performed and the
        /// original object is returned.
        /// </summary>
        /// <param name="dtype">
        /// The desired type
        /// </param>
        /// <param name="non_blocking">
        /// If True, and the source is in pinned memory
        /// and destination is on the GPU or vice versa, the copy is performed
        /// asynchronously with respect to the host. Otherwise, the argument
        /// has no effect.
        /// </param>
        public Tensor type(Dtype dtype = null, bool non_blocking = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (non_blocking!=false) kwargs["non_blocking"]=ToPython(non_blocking);
            dynamic py = __self__.InvokeMethod("type", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns this tensor cast to the type of the given tensor.
        /// 
        /// This is a no-op if the tensor is already of the correct type. This is
        /// equivalent to self.type(tensor.type())
        /// </summary>
        public Tensor type_as(Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("type_as", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor which contains all slices of size size from
        /// self tensor in the dimension dimension.
        /// 
        /// Step between two slices is given by step.
        /// 
        /// If sizedim is the size of dimension dimension for self, the size of
        /// dimension dimension in the returned tensor will be
        /// (sizedim - size) / step + 1.
        /// 
        /// An additional dimension of size size is appended in the returned tensor.
        /// </summary>
        /// <param name="dimension">
        /// dimension in which unfolding happens
        /// </param>
        /// <param name="size">
        /// the size of each slice that is unfolded
        /// </param>
        /// <param name="step">
        /// the step between each slice
        /// </param>
        public Tensor unfold(int dimension, int size, int step)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                dimension,
                size,
                step,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("unfold", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with numbers sampled from the continuous uniform
        /// distribution:
        /// 
        /// \[P(x) = \dfrac{1}{\text{to} - \text{from}}
        /// 
        /// \]
        /// </summary>
        public Tensor<T> uniform_<T>(T @from, T to)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                @from,
                to,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("uniform_", pyargs, kwargs);
            return ToCsharp<Tensor<T>>(py);
        }
        
        /// <summary>
        /// If self is a sparse COO tensor (i.e., with torch.sparse_coo layout),
        /// this returns a view of the contained values tensor. Otherwise, this throws an
        /// error.
        /// 
        /// See also Tensor.indices().
        /// 
        /// Note
        /// This method can only be called on a coalesced sparse tensor. See
        /// Tensor.coalesce() for details.
        /// </summary>
        public Tensor values()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("values");
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a new tensor with the same data as the self tensor but of a
        /// different shape.
        /// 
        /// The returned tensor shares the same data and must have the same number
        /// of elements, but may have a different size. For a tensor to be viewed, the new
        /// view size must be compatible with its original size and stride, i.e., each new
        /// view dimension must either be a subspace of an original dimension, or only span
        /// across original dimensions \(d, d+1, \dots, d+k\) that satisfy the following
        /// contiguity-like condition that \(\forall i = 0, \dots, k-1\),
        /// 
        /// \[\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]\]
        /// 
        /// Otherwise, contiguous() needs to be called before the tensor can be
        /// viewed. See also: reshape(), which returns a view if the shapes are
        /// compatible, and copies (equivalent to calling contiguous()) otherwise.
        /// </summary>
        public Tensor view(Shape shape)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                shape,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("view", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// View this tensor as the same size as other.
        /// self.view_as(other) is equivalent to self.view(other.size()).
        /// 
        /// Please see view() for more information about view.
        /// </summary>
        public Tensor view_as(Tensor other)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                other,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("view_as", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Fills self tensor with zeros.
        /// </summary>
        public Tensor zero_()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("zero_");
            return ToCsharp<Tensor>(py);
        }
        
    }
}
