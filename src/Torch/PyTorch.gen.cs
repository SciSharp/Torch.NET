// Code generated by CodeMinion: https://github.com/SciSharp/CodeMinion

using System;
using System.Collections;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using Python.Runtime;
using Numpy;
using Numpy.Models;

namespace Torch
{
    public partial class PyTorch
    {
        
        /// <summary>
        /// Returns True if obj is a PyTorch tensor.
        /// </summary>
        public bool is_tensor(Object obj)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                obj,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("is_tensor", pyargs, kwargs);
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Returns True if obj is a PyTorch storage object.
        /// </summary>
        public bool is_storage(Object obj)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                obj,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("is_storage", pyargs, kwargs);
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Returns True if the data type of tensor is a floating point data type i.e.,
        /// one of torch.float64, torch.float32 and torch.float16.
        /// </summary>
        public bool is_floating_point(Tensor tensor)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("is_floating_point", pyargs, kwargs);
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Sets the default floating point dtype to d. This type will be
        /// used as default floating point type for type inference in
        /// torch.tensor().
        /// 
        /// The default floating point dtype is initially torch.float32.
        /// </summary>
        public void set_default_dtype(Dtype d)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                d,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("set_default_dtype", pyargs, kwargs);
        }
        
        /// <summary>
        /// Get the current default floating point torch.dtype.
        /// </summary>
        public Dtype get_default_dtype()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("get_default_dtype");
            return ToCsharp<Dtype>(py);
        }
        
        /// <summary>
        /// Sets the default torch.Tensor type to floating point tensor type
        /// t. This type will also be used as default floating point type for
        /// type inference in torch.tensor().
        /// 
        /// The default floating point tensor type is initially torch.FloatTensor.
        /// </summary>
        public void set_default_tensor_type(Dtype t)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                t,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("set_default_tensor_type", pyargs, kwargs);
        }
        
        /// <summary>
        /// Returns the total number of elements in the input tensor.
        /// </summary>
        public int numel(Tensor input)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("numel", pyargs, kwargs);
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Set options for printing. Items shamelessly taken from NumPy
        /// </summary>
        /// <param name="precision">
        /// Number of digits of precision for floating point output
        /// (default = 4).
        /// </param>
        /// <param name="threshold">
        /// Total number of array elements which trigger summarization
        /// rather than full repr (default = 1000).
        /// </param>
        /// <param name="edgeitems">
        /// Number of array items in summary at beginning and end of
        /// each dimension (default = 3).
        /// </param>
        /// <param name="linewidth">
        /// The number of characters per line for the purpose of
        /// inserting line breaks (default = 80). Thresholded matrices will
        /// ignore this parameter.
        /// </param>
        /// <param name="profile">
        /// Sane defaults for pretty printing. Can override with any of
        /// the above options. (any one of default, short, full)
        /// </param>
        /// <param name="sci_mode">
        /// Enable (True) or disable (False) scientific notation. If
        /// None (default) is specified, the value is defined by _Formatter
        /// </param>
        public void set_printoptions(int precision =  4, int threshold =  1000, int edgeitems =  3, int linewidth =  80, string profile = "default", bool? sci_mode = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (precision!= 4) kwargs["precision"]=ToPython(precision);
            if (threshold!= 1000) kwargs["threshold"]=ToPython(threshold);
            if (edgeitems!= 3) kwargs["edgeitems"]=ToPython(edgeitems);
            if (linewidth!= 80) kwargs["linewidth"]=ToPython(linewidth);
            if (profile!="default") kwargs["profile"]=ToPython(profile);
            if (sci_mode!=null) kwargs["sci_mode"]=ToPython(sci_mode);
            dynamic py = __self__.InvokeMethod("set_printoptions", pyargs, kwargs);
        }
        
        /// <summary>
        /// Disables denormal floating numbers on CPU.
        /// 
        /// Returns True if your system supports flushing denormal numbers and it
        /// successfully configures flush denormal mode.  set_flush_denormal()
        /// is only supported on x86 architectures supporting SSE3.
        /// </summary>
        public bool set_flush_denormal(bool mode)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                mode,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("set_flush_denormal", pyargs, kwargs);
            return ToCsharp<bool>(py);
        }
        
        /// <summary>
        /// Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given indices
        /// with the given values. A sparse tensor can be uncoalesced, in that case, there are duplicate
        /// coordinates in the indices, and the value at that index is the sum of all duplicate value entries:
        /// torch.sparse.
        /// </summary>
        /// <param name="indices">
        /// Initial data for the tensor. Can be a list, tuple,
        /// NumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor
        /// internally. The indices are the coordinates of the non-zero values in the matrix, and thus
        /// should be two-dimensional where the first dimension is the number of tensor dimensions and
        /// the second dimension is the number of non-zero values.
        /// </param>
        /// <param name="values">
        /// Initial values for the tensor. Can be a list, tuple,
        /// NumPy ndarray, scalar, and other types.
        /// </param>
        /// <param name="size">
        /// Size of the sparse tensor. If not
        /// provided the size will be inferred as the minimum size big enough to hold all non-zero
        /// elements.
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, infers data type from values.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor sparse_coo_tensor(NDarray<int> indices, NDarray values, int? size = null, Dtype dtype = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                indices,
                values,
            });
            var kwargs=new PyDict();
            if (size!=null) kwargs["size"]=ToPython(size);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("sparse_coo_tensor", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Convert the data into a torch.Tensor. If the data is already a Tensor with the same dtype and device,
        /// no copy will be performed, otherwise a new Tensor will be returned with computational graph retained if data
        /// Tensor has requires_grad=True. Similarly, if the data is an ndarray of the corresponding dtype and
        /// the device is the cpu, no copy will be performed.
        /// </summary>
        /// <param name="data">
        /// Initial data for the tensor. Can be a list, tuple,
        /// NumPy ndarray, scalar, and other types.
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, infers data type from data.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        public Tensor as_tensor(NDarray data, Dtype dtype = null, Device device = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                data,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (device!=null) kwargs["device"]=ToPython(device);
            dynamic py = __self__.InvokeMethod("as_tensor", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Creates a Tensor from a numpy.ndarray.
        /// 
        /// The returned tensor and ndarray share the same memory. Modifications to
        /// the tensor will be reflected in the ndarray and vice versa. The returned
        /// tensor is not resizable.
        /// </summary>
        public Tensor from_numpy(NDarray ndarray)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                ndarray,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("from_numpy", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with the scalar value 0, with the shape defined
        /// by the variable argument sizes.
        /// </summary>
        /// <param name="sizes">
        /// a sequence of integers defining the shape of the output tensor.
        /// Can be a variable number of arguments or a collection like a list or tuple.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor zeros(Shape sizes, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("zeros", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with the scalar value 0, with the same size as
        /// input. torch.zeros_like(input) is equivalent to
        /// torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).
        /// 
        /// Warning
        /// As of 0.4, this function does not support an out keyword. As an alternative,
        /// the old torch.zeros_like(input, out=output) is equivalent to
        /// torch.zeros(input.size(), out=output).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor zeros_like(Tensor input, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("zeros_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with the scalar value 1, with the shape defined
        /// by the variable argument sizes.
        /// </summary>
        /// <param name="sizes">
        /// a sequence of integers defining the shape of the output tensor.
        /// Can be a variable number of arguments or a collection like a list or tuple.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor ones(Shape sizes, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("ones", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with the scalar value 1, with the same size as
        /// input. torch.ones_like(input) is equivalent to
        /// torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).
        /// 
        /// Warning
        /// As of 0.4, this function does not support an out keyword. As an alternative,
        /// the old torch.ones_like(input, out=output) is equivalent to
        /// torch.ones(input.size(), out=output).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor ones_like(Tensor input, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("ones_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a 1-D tensor of size \(\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor\)
        /// with values from the interval [start, end) taken with common difference
        /// step beginning from start.
        /// 
        /// Note that non-integer step is subject to floating point rounding errors when
        /// comparing against end; to avoid inconsistency, we advise adding a small epsilon to end
        /// in such cases.
        /// 
        /// \[\text{out}_{{i+1}} = \text{out}_{i} + \text{step}
        /// 
        /// \]
        /// </summary>
        /// <param name="end">
        /// the ending value for the set of points
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
        /// arguments. If any of start, end, or stop are floating-point, the
        /// dtype is inferred to be the default dtype, see
        /// get_default_dtype(). Otherwise, the dtype is inferred to
        /// be torch.int64.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor arange(double end, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                end,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("arange", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a 1-D tensor of size \(\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor\)
        /// with values from the interval [start, end) taken with common difference
        /// step beginning from start.
        /// 
        /// Note that non-integer step is subject to floating point rounding errors when
        /// comparing against end; to avoid inconsistency, we advise adding a small epsilon to end
        /// in such cases.
        /// 
        /// \[\text{out}_{{i+1}} = \text{out}_{i} + \text{step}
        /// 
        /// \]
        /// </summary>
        /// <param name="start">
        /// the starting value for the set of points. Default: 0.
        /// </param>
        /// <param name="end">
        /// the ending value for the set of points
        /// </param>
        /// <param name="step">
        /// the gap between each pair of adjacent points. Default: 1.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
        /// arguments. If any of start, end, or stop are floating-point, the
        /// dtype is inferred to be the default dtype, see
        /// get_default_dtype(). Otherwise, the dtype is inferred to
        /// be torch.int64.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor arange(double start, double end, double step = 1, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                start,
                end,
            });
            var kwargs=new PyDict();
            if (step!=1) kwargs["step"]=ToPython(step);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("arange", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a 1-D tensor of size \(\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1\)
        /// with values from start to end with step step. Step is
        /// the gap between two values in the tensor.
        /// 
        /// \[\text{out}_{i+1} = \text{out}_i + \text{step}.
        /// 
        /// \]
        /// 
        /// Warning
        /// This function is deprecated in favor of torch.arange().
        /// </summary>
        /// <param name="end">
        /// the ending value for the set of points
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
        /// arguments. If any of start, end, or stop are floating-point, the
        /// dtype is inferred to be the default dtype, see
        /// get_default_dtype(). Otherwise, the dtype is inferred to
        /// be torch.int64.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor range(float end, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                end,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("range", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a 1-D tensor of size \(\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1\)
        /// with values from start to end with step step. Step is
        /// the gap between two values in the tensor.
        /// 
        /// \[\text{out}_{i+1} = \text{out}_i + \text{step}.
        /// 
        /// \]
        /// 
        /// Warning
        /// This function is deprecated in favor of torch.arange().
        /// </summary>
        /// <param name="start">
        /// the starting value for the set of points. Default: 0.
        /// </param>
        /// <param name="end">
        /// the ending value for the set of points
        /// </param>
        /// <param name="step">
        /// the gap between each pair of adjacent points. Default: 1.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input
        /// arguments. If any of start, end, or stop are floating-point, the
        /// dtype is inferred to be the default dtype, see
        /// get_default_dtype(). Otherwise, the dtype is inferred to
        /// be torch.int64.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor range(float start, float end, float step = 1f, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                start,
                end,
            });
            var kwargs=new PyDict();
            if (step!=1f) kwargs["step"]=ToPython(step);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("range", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a one-dimensional tensor of steps
        /// equally spaced points between start and end.
        /// 
        /// The output tensor is 1-D of size steps.
        /// </summary>
        /// <param name="start">
        /// the starting value for the set of points
        /// </param>
        /// <param name="end">
        /// the ending value for the set of points
        /// </param>
        /// <param name="steps">
        /// number of points to sample between start
        /// and end. Default: 100.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor linspace(float start, float end, int steps = 100, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                start,
                end,
            });
            var kwargs=new PyDict();
            if (steps!=100) kwargs["steps"]=ToPython(steps);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("linspace", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a one-dimensional tensor of steps points
        /// logarithmically spaced with base base between
        /// \({\text{base}}^{\text{start}}\) and \({\text{base}}^{\text{end}}\).
        /// 
        /// The output tensor is 1-D of size steps.
        /// </summary>
        /// <param name="start">
        /// the starting value for the set of points
        /// </param>
        /// <param name="end">
        /// the ending value for the set of points
        /// </param>
        /// <param name="steps">
        /// number of points to sample between start
        /// and end. Default: 100.
        /// </param>
        /// <param name="@base">
        /// base of the logarithm function. Default: 10.0.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor logspace(float start, float end, int steps = 100, float @base = 10.0f, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                start,
                end,
            });
            var kwargs=new PyDict();
            if (steps!=100) kwargs["steps"]=ToPython(steps);
            if (@base!=10.0f) kwargs["base"]=ToPython(@base);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("logspace", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
        /// </summary>
        /// <param name="n">
        /// the number of rows
        /// </param>
        /// <param name="m">
        /// the number of columns with default being n
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor eye(int n, int? m = null, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                n,
            });
            var kwargs=new PyDict();
            if (m!=null) kwargs["m"]=ToPython(m);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("eye", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with uninitialized data. The shape of the tensor is
        /// defined by the variable argument sizes.
        /// </summary>
        /// <param name="sizes">
        /// a sequence of integers defining the shape of the output tensor.
        /// Can be a variable number of arguments or a collection like a list or tuple.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        /// <param name="pin_memory">
        /// If set, returned tensor would be allocated in
        /// the pinned memory. Works only for CPU tensors. Default: False.
        /// </param>
        public Tensor empty(Shape sizes, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false, bool? pin_memory = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            if (pin_memory!=null) kwargs["pin_memory"]=ToPython(pin_memory);
            dynamic py = __self__.InvokeMethod("empty", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns an uninitialized tensor with the same size as input.
        /// torch.empty_like(input) is equivalent to
        /// torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor empty_like(Tensor input, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("empty_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor of size size filled with fill_value.
        /// </summary>
        /// <param name="size">
        /// a list, tuple, or torch.Size of integers defining the
        /// shape of the output tensor.
        /// </param>
        /// <param name="fill_value">
        /// the number to fill the output tensor with.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor full(Shape size, object fill_value, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                size,
                fill_value,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("full", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor with the same size as input filled with fill_value.
        /// torch.full_like(input, fill_value) is equivalent to
        /// torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="fill_value">
        /// the number to fill the output tensor with.
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor full_like(Tensor input, object fill_value, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                fill_value,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("full_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Concatenates the given sequence of seq tensors in the given dimension.
        /// All tensors must either have the same shape (except in the concatenating
        /// dimension) or be empty.
        /// 
        /// torch.cat() can be seen as an inverse operation for torch.split()
        /// and torch.chunk().
        /// 
        /// torch.cat() can be best understood via examples.
        /// </summary>
        /// <param name="tensors">
        /// any python sequence of tensors of the same type.
        /// Non-empty tensors provided must have the same shape, except in the
        /// cat dimension.
        /// </param>
        /// <param name="dim">
        /// the dimension over which the tensors are concatenated
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor cat(Tensor[] tensors, int? dim = 0, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensors,
            });
            var kwargs=new PyDict();
            if (dim!=null) kwargs["dim"]=ToPython(dim);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("cat", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Splits a tensor into a specific number of chunks.
        /// 
        /// Last chunk will be smaller if the tensor size along the given dimension
        /// dim is not divisible by chunks.
        /// </summary>
        /// <param name="tensor">
        /// the tensor to split
        /// </param>
        /// <param name="chunks">
        /// number of chunks to return
        /// </param>
        /// <param name="dim">
        /// dimension along which to split the tensor
        /// </param>
        public Tensor[] chunk(Tensor tensor, int chunks, int dim = 0)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
                chunks,
            });
            var kwargs=new PyDict();
            if (dim!=0) kwargs["dim"]=ToPython(dim);
            dynamic py = __self__.InvokeMethod("chunk", pyargs, kwargs);
            return ToCsharp<Tensor[]>(py);
        }
        
        /// <summary>
        /// Gathers values along an axis specified by dim.
        /// 
        /// For a 3-D tensor the output is specified by:
        /// 
        /// out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
        /// out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
        /// out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
        /// 
        /// If input is an n-dimensional tensor with size
        /// \((x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})\)
        /// and dim = i, then index must be an \(n\)-dimensional tensor with
        /// size \((x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})\) where \(y \geq 1\)
        /// and out will have the same size as index.
        /// </summary>
        /// <param name="input">
        /// the source tensor
        /// </param>
        /// <param name="dim">
        /// the axis along which to index
        /// </param>
        /// <param name="index">
        /// the indices of elements to gather
        /// </param>
        /// <param name="@out">
        /// the destination tensor
        /// </param>
        /// <param name="sparse_grad">
        /// If True, gradient w.r.t. input will be a sparse tensor.
        /// </param>
        public Tensor gather(Tensor input, int dim, Tensor<long> index, Tensor @out = null, bool? sparse_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                dim,
                index,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (sparse_grad!=null) kwargs["sparse_grad"]=ToPython(sparse_grad);
            dynamic py = __self__.InvokeMethod("gather", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a new tensor which indexes the input tensor along dimension
        /// dim using the entries in index which is a LongTensor.
        /// 
        /// The returned tensor has the same number of dimensions as the original tensor
        /// (input).  The dimth dimension has the same size as the length
        /// of index; other dimensions have the same size as in the original tensor.
        /// 
        /// Note
        /// The returned tensor does not use the same storage as the original
        /// tensor.  If out has a different shape than expected, we
        /// silently change it to the correct shape, reallocating the underlying
        /// storage if necessary.
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="dim">
        /// the dimension in which we index
        /// </param>
        /// <param name="index">
        /// the 1-D tensor containing the indices to index
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor index_select(Tensor input, int dim, Tensor<long> index, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                dim,
                index,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("index_select", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a new 1-D tensor which indexes the input tensor according to
        /// the binary mask mask which is a ByteTensor.
        /// 
        /// The shapes of the mask tensor and the input tensor dont need
        /// to match, but they must be broadcastable.
        /// 
        /// Note
        /// The returned tensor does not use the same storage
        /// as the original tensor
        /// </summary>
        /// <param name="input">
        /// the input data
        /// </param>
        /// <param name="mask">
        /// the tensor containing the binary mask to index with
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor masked_select(Tensor input, Tensor<byte> mask, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                mask,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("masked_select", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a new tensor that is a narrowed version of input tensor. The
        /// dimension dim is input from start to start + length. The
        /// returned tensor and input tensor share the same underlying storage.
        /// </summary>
        /// <param name="input">
        /// the tensor to narrow
        /// </param>
        /// <param name="dimension">
        /// the dimension along which to narrow
        /// </param>
        /// <param name="start">
        /// the starting dimension
        /// </param>
        /// <param name="length">
        /// the distance to the ending dimension
        /// </param>
        public Tensor narrow(Tensor input, int dimension, int start, int length)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                dimension,
                start,
                length,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("narrow", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor containing the indices of all non-zero elements of
        /// input.  Each row in the result contains the indices of a non-zero
        /// element in input. The result is sorted lexicographically, with
        /// the last index changing the fastest (C-style).
        /// 
        /// If input has n dimensions, then the resulting indices tensor
        /// out is of size \((z \times n)\), where \(z\) is the total number of
        /// non-zero elements in the input tensor.
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="@out">
        /// the output tensor containing indices
        /// </param>
        public Tensor<long> nonzero(Tensor input, Tensor<long> @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("nonzero", pyargs, kwargs);
            return ToCsharp<Tensor<long>>(py);
        }
        
        /// <summary>
        /// Returns a tensor with the same data and number of elements as input,
        /// but with the specified shape. When possible, the returned tensor will be a view
        /// of input. Otherwise, it will be a copy. Contiguous inputs and inputs
        /// with compatible strides can be reshaped without copying, but you should not
        /// depend on the copying vs. viewing behavior.
        /// 
        /// See torch.Tensor.view() on when it is possible to return a view.
        /// 
        /// A single dimension may be -1, in which case its inferred from the remaining
        /// dimensions and the number of elements in input.
        /// </summary>
        /// <param name="input">
        /// the tensor to be reshaped
        /// </param>
        /// <param name="shape">
        /// the new shape
        /// </param>
        public Tensor reshape(Tensor input, Shape shape)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                shape,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("reshape", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Splits the tensor into chunks.
        /// 
        /// If split_size_or_sections is an integer type, then tensor will
        /// be split into equally sized chunks (if possible). Last chunk will be smaller if
        /// the tensor size along the given dimension dim is not divisible by
        /// split_size.
        /// 
        /// If split_size_or_sections is a list, then tensor will be split
        /// into len(split_size_or_sections) chunks with sizes in dim according
        /// to split_size_or_sections.
        /// </summary>
        /// <param name="tensor">
        /// tensor to split.
        /// </param>
        /// <param name="split_size_or_sections">
        /// size of a single chunk or
        /// list of sizes for each chunk
        /// </param>
        /// <param name="dim">
        /// dimension along which to split the tensor.
        /// </param>
        public void split(Tensor tensor, int split_size_or_sections, int dim = 0)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
                split_size_or_sections,
            });
            var kwargs=new PyDict();
            if (dim!=0) kwargs["dim"]=ToPython(dim);
            dynamic py = __self__.InvokeMethod("split", pyargs, kwargs);
        }
        
        /// <summary>
        /// Returns a tensor with all the dimensions of input of size 1 removed.
        /// 
        /// For example, if input is of shape:
        /// \((A \times 1 \times B \times C \times 1 \times D)\) then the out tensor
        /// will be of shape: \((A \times B \times C \times D)\).
        /// 
        /// When dim is given, a squeeze operation is done only in the given
        /// dimension. If input is of shape: \((A \times 1 \times B)\),
        /// squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1)
        /// will squeeze the tensor to the shape \((A \times B)\).
        /// 
        /// Note
        /// The returned tensor shares the storage with the input tensor,
        /// so changing the contents of one will change the contents of the other.
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="dim">
        /// if given, the input will be squeezed only in
        /// this dimension
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor squeeze(Tensor input, int? dim = null, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (dim!=null) kwargs["dim"]=ToPython(dim);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("squeeze", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Concatenates sequence of tensors along a new dimension.
        /// 
        /// All tensors need to be of the same size.
        /// </summary>
        /// <param name="seq">
        /// sequence of tensors to concatenate
        /// </param>
        /// <param name="dim">
        /// dimension to insert. Has to be between 0 and the number
        /// of dimensions of concatenated tensors (inclusive)
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor stack(Tensor[] seq, int dim = 0, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                seq,
            });
            var kwargs=new PyDict();
            if (dim!=0) kwargs["dim"]=ToPython(dim);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("stack", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Expects input to be &lt;= 2-D tensor and transposes dimensions 0
        /// and 1.
        /// 
        /// 0-D and 1-D tensors are returned as it is and
        /// 2-D tensor can be seen as a short-hand function for transpose(input, 0, 1).
        /// </summary>
        public Tensor t(Tensor input)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("t", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a new tensor with the elements of input at the given indices.
        /// The input tensor is treated as if it were viewed as a 1-D tensor. The result
        /// takes the same shape as the indices.
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="indices">
        /// the indices into tensor
        /// </param>
        public Tensor take(Tensor input, Tensor<long> indices)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                indices,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("take", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor that is a transposed version of input.
        /// The given dimensions dim0 and dim1 are swapped.
        /// 
        /// The resulting out tensor shares its underlying storage with the
        /// input tensor, so changing the content of one would change the content
        /// of the other.
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="dim0">
        /// the first dimension to be transposed
        /// </param>
        /// <param name="dim1">
        /// the second dimension to be transposed
        /// </param>
        public Tensor transpose(Tensor input, int dim0, int dim1)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                dim0,
                dim1,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("transpose", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Removes a tensor dimension.
        /// 
        /// Returns a tuple of all slices along a given dimension, already without it.
        /// </summary>
        /// <param name="tensor">
        /// the tensor to unbind
        /// </param>
        /// <param name="dim">
        /// dimension to remove
        /// </param>
        public Tensor[] unbind(Tensor tensor, int dim = 0)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                tensor,
            });
            var kwargs=new PyDict();
            if (dim!=0) kwargs["dim"]=ToPython(dim);
            dynamic py = __self__.InvokeMethod("unbind", pyargs, kwargs);
            return ToCsharp<Tensor[]>(py);
        }
        
        /// <summary>
        /// Returns a new tensor with a dimension of size one inserted at the
        /// specified position.
        /// 
        /// The returned tensor shares the same underlying data with this tensor.
        /// 
        /// A dim value within the range [-input.dim() - 1, input.dim() + 1)
        /// can be used. Negative dim will correspond to unsqueeze()
        /// applied at dim = dim + input.dim() + 1.
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="dim">
        /// the index at which to insert the singleton dimension
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor unsqueeze(Tensor input, int dim, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                dim,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("unsqueeze", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Return a tensor of elements selected from either x or y, depending on condition.
        /// 
        /// The operation is defined as:
        /// 
        /// \[out_i = \begin{cases}
        ///     x_i & \text{if } \text{condition}_i \\
        ///     y_i & \text{otherwise} \\
        /// \end{cases}
        /// 
        /// \]
        /// 
        /// Note
        /// The tensors condition, x, y must be broadcastable.
        /// </summary>
        /// <param name="condition">
        /// When True (nonzero), yield x, otherwise yield y
        /// </param>
        /// <param name="x">
        /// values selected at indices where condition is True
        /// </param>
        /// <param name="y">
        /// values selected at indices where condition is False
        /// </param>
        public Tensor @where(Tensor<byte> condition, Tensor x, Tensor y)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                condition,
                x,
                y,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("where", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Sets the seed for generating random numbers. Returns a
        /// torch._C.Generator object.
        /// </summary>
        public void manual_seed(int seed)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                seed,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("manual_seed", pyargs, kwargs);
        }
        
        /// <summary>
        /// Returns the initial seed for generating random numbers as a
        /// Python long.
        /// </summary>
        public void initial_seed()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("initial_seed");
        }
        
        /// <summary>
        /// Returns the random number generator state as a torch.ByteTensor.
        /// </summary>
        public void get_rng_state()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("get_rng_state");
        }
        
        /// <summary>
        /// Sets the random number generator state.
        /// </summary>
        public void set_rng_state(Tensor<byte> new_state)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                new_state,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("set_rng_state", pyargs, kwargs);
        }
        
        /// <summary>
        /// Draws binary random numbers (0 or 1) from a Bernoulli distribution.
        /// 
        /// The input tensor should be a tensor containing probabilities
        /// to be used for drawing the binary random number.
        /// Hence, all values in input have to be in the range:
        /// \(0 \leq \text{input}_i \leq 1\).
        /// 
        /// The \(\text{i}^{th}\) element of the output tensor will draw a
        /// value \(1\) according to the \(\text{i}^{th}\) probability value given
        /// in input.
        /// 
        /// \[\text{out}_{i} \sim \mathrm{Bernoulli}(p = \text{input}_{i})
        /// 
        /// \]
        /// 
        /// The returned out tensor only has values 0 or 1 and is of the same
        /// shape as input.
        /// 
        /// out can have integral dtype, but input must have floating
        /// point dtype.
        /// </summary>
        /// <param name="input">
        /// the input tensor of probability values for the Bernoulli distribution
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor bernoulli(Tensor input, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("bernoulli", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor where each row contains num_samples indices sampled
        /// from the multinomial probability distribution located in the corresponding row
        /// of tensor input.
        /// 
        /// Note
        /// The rows of input do not need to sum to one (in which case we use
        /// the values as weights), but must be non-negative, finite and have
        /// a non-zero sum.
        /// 
        /// Indices are ordered from left to right according to when each was sampled
        /// (first samples are placed in first column).
        /// 
        /// If input is a vector, out is a vector of size num_samples.
        /// 
        /// If input is a matrix with m rows, out is an matrix of shape
        /// \((m \times \text{num\_samples})\).
        /// 
        /// If replacement is True, samples are drawn with replacement.
        /// 
        /// If not, they are drawn without replacement, which means that when a
        /// sample index is drawn for a row, it cannot be drawn again for that row.
        /// 
        /// Note
        /// When drawn without replacement, num_samples must be lower than
        /// number of non-zero elements in input (or the min number of non-zero
        /// elements in each row of input if it is a matrix).
        /// </summary>
        /// <param name="input">
        /// the input tensor containing probabilities
        /// </param>
        /// <param name="num_samples">
        /// number of samples to draw
        /// </param>
        /// <param name="replacement">
        /// whether to draw with replacement or not
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor<long> multinomial(Tensor input, int num_samples, bool? replacement = false, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                num_samples,
            });
            var kwargs=new PyDict();
            if (replacement!=null) kwargs["replacement"]=ToPython(replacement);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("multinomial", pyargs, kwargs);
            return ToCsharp<Tensor<long>>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with random numbers from a uniform distribution
        /// on the interval \([0, 1)\)
        /// 
        /// The shape of the tensor is defined by the variable argument sizes.
        /// </summary>
        /// <param name="sizes">
        /// a sequence of integers defining the shape of the output tensor.
        /// Can be a variable number of arguments or a collection like a list or tuple.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor rand(Shape sizes, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("rand", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor with the same size as input that is filled with
        /// random numbers from a uniform distribution on the interval \([0, 1)\).
        /// torch.rand_like(input) is equivalent to
        /// torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor rand_like(Tensor input, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("rand_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with random integers generated uniformly
        /// between low (inclusive) and high (exclusive).
        /// 
        /// The shape of the tensor is defined by the variable argument size.
        /// </summary>
        /// <param name="high">
        /// One above the highest integer to be drawn from the distribution.
        /// </param>
        /// <param name="size">
        /// a tuple defining the shape of the output tensor.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor randint(int high, Shape size, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                high,
                size,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randint", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with random integers generated uniformly
        /// between low (inclusive) and high (exclusive).
        /// 
        /// The shape of the tensor is defined by the variable argument size.
        /// </summary>
        /// <param name="low">
        /// Lowest integer to be drawn from the distribution. Default: 0.
        /// </param>
        /// <param name="high">
        /// One above the highest integer to be drawn from the distribution.
        /// </param>
        /// <param name="size">
        /// a tuple defining the shape of the output tensor.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor randint(int low, int high, Shape size, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
            });
            var kwargs=new PyDict();
            if (low!=null) kwargs["low"]=ToPython(low);
            if (high!=null) kwargs["high"]=ToPython(high);
            if (size!=null) kwargs["size"]=ToPython(size);
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randint", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor with the same shape as Tensor input filled with
        /// random integers generated uniformly between low (inclusive) and
        /// high (exclusive).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="high">
        /// One above the highest integer to be drawn from the distribution.
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor randint_like(Tensor input, int high, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
                high,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randint_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor with the same shape as Tensor input filled with
        /// random integers generated uniformly between low (inclusive) and
        /// high (exclusive).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="low">
        /// Lowest integer to be drawn from the distribution. Default: 0.
        /// </param>
        /// <param name="high">
        /// One above the highest integer to be drawn from the distribution.
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor randint_like(Tensor input, int low, int high, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (low!=null) kwargs["low"]=ToPython(low);
            if (high!=null) kwargs["high"]=ToPython(high);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randint_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor filled with random numbers from a normal distribution
        /// with mean 0 and variance 1 (also called the standard normal
        /// distribution).
        /// 
        /// \[\text{out}_{i} \sim \mathcal{N}(0, 1)
        /// 
        /// \]
        /// 
        /// The shape of the tensor is defined by the variable argument sizes.
        /// </summary>
        /// <param name="sizes">
        /// a sequence of integers defining the shape of the output tensor.
        /// Can be a variable number of arguments or a collection like a list or tuple.
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: if None, uses a global default (see torch.set_default_tensor_type()).
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor randn(Shape sizes, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                sizes,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randn", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a tensor with the same size as input that is filled with
        /// random numbers from a normal distribution with mean 0 and variance 1.
        /// torch.randn_like(input) is equivalent to
        /// torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).
        /// </summary>
        /// <param name="input">
        /// the size of input will determine size of the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned Tensor.
        /// Default: if None, defaults to the dtype of input.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned tensor.
        /// Default: if None, defaults to the layout of input.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, defaults to the device of input.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor randn_like(Tensor input, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randn_like", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a random permutation of integers from 0 to n - 1.
        /// </summary>
        /// <param name="n">
        /// the upper bound (exclusive)
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        /// <param name="dtype">
        /// the desired data type of returned tensor.
        /// Default: torch.int64.
        /// </param>
        /// <param name="layout">
        /// the desired layout of returned Tensor.
        /// Default: torch.strided.
        /// </param>
        /// <param name="device">
        /// the desired device of returned tensor.
        /// Default: if None, uses the current device for the default tensor type
        /// (see torch.set_default_tensor_type()). device will be the CPU
        /// for CPU tensor types and the current CUDA device for CUDA tensor types.
        /// </param>
        /// <param name="requires_grad">
        /// If autograd should record operations on the
        /// returned tensor. Default: False.
        /// </param>
        public Tensor<long> randperm(int n, Tensor @out = null, Dtype dtype = null, Layout layout = null, Device device = null, bool? requires_grad = false)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                n,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            if (dtype!=null) kwargs["dtype"]=ToPython(dtype);
            if (layout!=null) kwargs["layout"]=ToPython(layout);
            if (device!=null) kwargs["device"]=ToPython(device);
            if (requires_grad!=null) kwargs["requires_grad"]=ToPython(requires_grad);
            dynamic py = __self__.InvokeMethod("randperm", pyargs, kwargs);
            return ToCsharp<Tensor<long>>(py);
        }
        
        /// <summary>
        /// Saves an object to a disk file.
        /// 
        /// See also: Recommended approach for saving a model
        /// </summary>
        /// <param name="obj">
        /// saved object
        /// </param>
        /// <param name="f">
        /// a file-like object (has to implement write and flush) or a string
        /// containing a file name
        /// </param>
        /// <param name="pickle_module">
        /// module used for pickling metadata and objects
        /// </param>
        /// <param name="pickle_protocol">
        /// can be specified to override the default protocol
        /// </param>
        public void save(PythonObject obj, string f, PyObject pickle_module = null, int pickle_protocol = 2)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                obj,
                f,
            });
            var kwargs=new PyDict();
            if (pickle_module!=null) kwargs["pickle_module"]=ToPython(pickle_module);
            if (pickle_protocol!=2) kwargs["pickle_protocol"]=ToPython(pickle_protocol);
            dynamic py = __self__.InvokeMethod("save", pyargs, kwargs);
        }
        
        /// <summary>
        /// Loads an object saved with torch.save() from a file.
        /// 
        /// torch.load() uses Pythons unpickling facilities but treats storages,
        /// which underlie tensors, specially. They are first deserialized on the
        /// CPU and are then moved to the device they were saved from. If this fails
        /// (e.g. because the run time system doesnt have certain devices), an exception
        /// is raised. However, storages can be dynamically remapped to an alternative
        /// set of devices using the map_location argument.
        /// 
        /// If map_location is a callable, it will be called once for each serialized
        /// storage with two arguments: storage and location. The storage argument
        /// will be the initial deserialization of the storage, residing on the CPU.
        /// Each serialized storage has a location tag associated with it which
        /// identifies the device it was saved from, and this tag is the second
        /// argument passed to map_location. The builtin location tags are cpu for
        /// CPU tensors and cuda:device_id (e.g. cuda:2) for CUDA tensors.
        /// map_location should return either None or a storage. If map_location returns
        /// a storage, it will be used as the final deserialized object, already moved to
        /// the right device. Otherwise, \(torch.load\) will fall back to the default
        /// behavior, as if map_location wasnt specified.
        /// 
        /// If map_location is a string, it should be a device tag, where all tensors
        /// should be loaded.
        /// 
        /// Otherwise, if map_location is a dict, it will be used to remap location tags
        /// appearing in the file (keys), to ones that specify where to put the
        /// storages (values).
        /// 
        /// User extensions can register their own location tags and tagging and
        /// deserialization methods using register_package.
        /// </summary>
        /// <param name="f">
        /// a file-like object (has to implement read, readline, tell, and seek),
        /// or a string containing a file name
        /// </param>
        /// <param name="map_location">
        /// a function, torch.device, string or a dict specifying how to remap storage
        /// locations
        /// </param>
        /// <param name="pickle_module">
        /// module used for unpickling metadata and objects (has to
        /// match the pickle_module used to serialize file)
        /// </param>
        /// <param name="pickle_load_args">
        /// optional keyword arguments passed over to
        /// pickle_module.load and pickle_module.Unpickler, e.g.,
        /// encoding=....
        /// </param>
        public void load(string f, PyObject map_location = null, PyObject pickle_module = null, params PyObject[] pickle_load_args)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                f,
            });
            var kwargs=new PyDict();
            if (map_location!=null) kwargs["map_location"]=ToPython(map_location);
            if (pickle_module!=null) kwargs["pickle_module"]=ToPython(pickle_module);
            if (pickle_load_args!=null) kwargs["pickle_load_args"]=ToPython(pickle_load_args);
            dynamic py = __self__.InvokeMethod("load", pyargs, kwargs);
        }
        
        /// <summary>
        /// Gets the number of threads used for parallelizing CPU operations
        /// </summary>
        public int get_num_threads()
        {
            //auto-generated code, do not change
            var __self__=self;
            dynamic py = __self__.InvokeMethod("get_num_threads");
            return ToCsharp<int>(py);
        }
        
        /// <summary>
        /// Sets the number of threads used for parallelizing CPU operations.
        /// WARNING:
        /// To ensure that the correct number of threads is used, set_num_threads
        /// must be called before running eager, JIT or autograd code.
        /// </summary>
        public void set_num_threads(int num)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                num,
            });
            var kwargs=new PyDict();
            dynamic py = __self__.InvokeMethod("set_num_threads", pyargs, kwargs);
        }
        
        /// <summary>
        /// Computes the element-wise absolute value of the given input tensor.
        /// 
        /// \[\text{out}_{i} = |\text{input}_{i}|
        /// 
        /// \]
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor abs(Tensor input, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("abs", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
        /// <summary>
        /// Returns a new tensor with the arccosine  of the elements of input.
        /// 
        /// \[\text{out}_{i} = \cos^{-1}(\text{input}_{i})
        /// 
        /// \]
        /// </summary>
        /// <param name="input">
        /// the input tensor
        /// </param>
        /// <param name="@out">
        /// the output tensor
        /// </param>
        public Tensor acos(Tensor input, Tensor @out = null)
        {
            //auto-generated code, do not change
            var __self__=self;
            var pyargs=ToTuple(new object[]
            {
                input,
            });
            var kwargs=new PyDict();
            if (@out!=null) kwargs["out"]=ToPython(@out);
            dynamic py = __self__.InvokeMethod("acos", pyargs, kwargs);
            return ToCsharp<Tensor>(py);
        }
        
    }
}
